{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re as re\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datafile\n",
    "data_file = \"../LazadaData/Lazada_AllProducts_160220.csv\"\n",
    "data = pd.read_csv(data_file)\n",
    "data.columns = data.columns.str.strip().str.replace(\" \",\"_\") # reason for doing this is cause cannot get columns with space into a list\n",
    "\n",
    "#reformat the respective columns\n",
    "\n",
    "#lower case\n",
    "data['Review'] = [word.lower() for word in data['Review']]\n",
    "\n",
    "#Strip off the Variation Word\n",
    "data['Product_Purchase'] = data['Product_Purchase'].str.strip(\"Variation:\")\n",
    "\n",
    "#Change Date of Review to DateTime\n",
    "#data['Date_Of_Review'] = pd.to_datetime(data['Date_Of_Review'])\n",
    "\n",
    "data.info()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation and Initalise of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the old csv data into a list\n",
    "row_id_list = data['Unnamed:_0'].tolist()\n",
    "brand_list = data['Brand'].tolist()\n",
    "category_list = data['Category'].tolist()\n",
    "product_name_list = data['Product_Name'].tolist()\n",
    "price_list = data['Price'].tolist()\n",
    "reviewer_list = data['Reviewer'].tolist()\n",
    "review_list = data['Review'].tolist()\n",
    "product_variation_list = data['Product_Purchase'].tolist()\n",
    "rating_list = data['Ratings'].tolist()\n",
    "date_review_list = data[\"Date_Of_Review\"].tolist()\n",
    "response_list = data[\"Response\"].tolist()\n",
    "\n",
    "# initialising the new columns\n",
    "id_csv=[]\n",
    "brand_csv = []\n",
    "category_csv = []\n",
    "product_name_csv = []\n",
    "prices_csv = []\n",
    "reviewer_csv =[]\n",
    "review_csv = []\n",
    "product_variation_csv = []\n",
    "rating_csv = []\n",
    "date_review_csv = []\n",
    "response_csv = []\n",
    "\n",
    "# this is the length of the csv old data\n",
    "print (\"The length of the dataset is:\", len(brand_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload the appos\n",
    "from word_dict import appos\n",
    "\n",
    "print(appos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the Date Format [Optional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'brand_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b55b61d3c361>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnew_date\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbrand_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mdate_review\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdate_review_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'days ago'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdate_review\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'day ago'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdate_review\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'brand_list' is not defined"
     ]
    }
   ],
   "source": [
    "new_date =[]\n",
    "for i in range(len(brand_list)):\n",
    "    \n",
    "    date_review = date_review_list[i]\n",
    "    \n",
    "    if platform_list[i] == 'Lazada':\n",
    "        if 'days ago' in date_review or 'day ago' in date_review :\n",
    "            day = date_review.replace('days ago', '').replace('day ago', '')\n",
    "            date_review = datetime.today() - timedelta(days=int(day))\n",
    "            date_review = date_review.strftime(\"%d-%b-%y\")\n",
    "#             print(\"trans\",date_review)\n",
    "\n",
    "        elif 'weeks ago' in date_review or 'week ago' in date_review:\n",
    "            day = date_review.replace('weeks ago', '').replace('week ago', '')\n",
    "            day = int(day)*7\n",
    "            date_review = datetime.today() - timedelta(days=day)\n",
    "            date_review = date_review.strftime(\"%d-%b-%y\")\n",
    "#             print(\"trans\",date_review)\n",
    "\n",
    "        elif 'hours ago' in date_review or 'hour ago' in date_review:\n",
    "            hours = date_review.replace('hours ago', '').replace('hour ago', '')\n",
    "            date_review = datetime.today() - timedelta(hours=int(hours))\n",
    "            date_review = date_review.strftime(\"%d-%b-%y\")\n",
    "    \n",
    "#         print(\"trans\",date_review)\n",
    "        new_date.append(date_review)\n",
    "    else:\n",
    "#         print(platform_list[i])\n",
    "        new_date.append(date_review)\n",
    "        \n",
    "data[\"Date_Of_Review\"] = new_date\n",
    "\n",
    "new_date_review_list = data[\"Date_Of_Review\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spilt the Sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "for i in range(len(brand_list)):\n",
    "    # current_review is a string\n",
    "    current_review = review_list[i]\n",
    "    #print(current_review)\n",
    "    \n",
    "    #spilt the sentence\n",
    "    splitted_sentence= re.split(r'[.!]', current_review) \n",
    "    print(splitted_sentence)\n",
    "    \n",
    "    for j in range(len(splitted_sentence)):\n",
    "        \n",
    "        splitted_sentence[j].strip()\n",
    "        if len(splitted_sentence[j]) > 0 :\n",
    "            \n",
    "            processed_review = splitted_sentence[j].strip()\n",
    "            \n",
    "            # Negation handling\n",
    "            processed_review = processed_review.split()\n",
    "            processed_review =[appos[w] if w in appos else w for w in processed_review]\n",
    "            processed_review = \" \".join(processed_review) \n",
    "            \n",
    "            processed_review = processed_review.strip()\n",
    "            # Remove all the special characters\n",
    "            processed_review = re.sub(r'\\W', ' ', processed_review)\n",
    "            # Removing prefixed 'b'\n",
    "            processed_review = re.sub(r'^b\\s+', '', processed_review)\n",
    "            # remove all single characters contains a white space character\n",
    "            processed_review= re.sub(r'^[a-zA-Z]$', ' ', processed_review)\n",
    "            # Substituting multiple spaces with single space\n",
    "            processed_review = re.sub(r'\\s+', ' ', processed_review, flags=re.I)\n",
    "          \n",
    "            \n",
    "            processed = processed_review.strip()\n",
    "            print(processed)\n",
    "            \n",
    "            # final check if the processed_review is empty\n",
    "            final = []\n",
    "            if processed != \"\":\n",
    "                tokenized = word_tokenize(processed)\n",
    "                #removed additional letters eg \"gooooooodddddd\" to \"good\"\n",
    "                for w in tokenized:\n",
    "                    processed_2 = reduce_lengthening(w)\n",
    "                    processed_2 = spell.correction(processed_2)\n",
    "                    final.append(processed_2)\n",
    "                final =' '.join(final)\n",
    "                print(final)\n",
    "                # append into a dictionary\n",
    "                id_csv.append(row_id_list[i])\n",
    "                brand_csv.append(brand_list[i])\n",
    "                category_csv.append(category_list[i])\n",
    "                product_name_csv.append(product_name_list[i])\n",
    "                prices_csv.append(price_list[i])\n",
    "                reviewer_csv.append(reviewer_list[i])\n",
    "                review_csv.append(final)\n",
    "                product_variation_csv.append(product_variation_list[i])\n",
    "                rating_csv.append(rating_list[i])\n",
    "                date_review_csv.append(new_date_review_list[i])\n",
    "                response_csv.append(response_list[i])\n",
    "\n",
    "            \n",
    "#Store into a new data frame\n",
    "splitted_data = {'ID':id_csv ,'Brand':brand_csv, 'Category': category_csv, 'Product Name ': product_name_csv, 'Price':prices_csv ,'Reviewer':reviewer_csv,'Review':review_csv, 'Product Purchase':product_variation_csv,'Ratings':rating_csv,'Date Of Review':date_review_csv,'Response': response_csv }\n",
    "splitted_df = pd.DataFrame.from_dict(splitted_data)\n",
    "# splitted_df.to_csv('Lazada_process_v1.csv')\n",
    "splitted_df.head()\n",
    "splitted_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Training Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "def lemmatization(texts):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "review_cleaned=[]\n",
    "stop_list = stopwords.words('english')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "sp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "splitted_df= splitted_df[splitted_df.Review != 'no comments review is an image']\n",
    "tokenized_review = splitted_df[\"Review\"]\n",
    "\n",
    "\n",
    "for sentence in tokenized_review:\n",
    "    tokenized_review = nltk.word_tokenize(sentence)\n",
    "    review_stopremoved = [w for w in tokenized_review if w not in stop_list]\n",
    "    #print(review_stopremoved)\n",
    "    review_cleaned.append(review_stopremoved)\n",
    "    \n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(review_cleaned, allowed_postags=['NOUN', 'ADJ','VERB'])\n",
    "\n",
    "## Find a way to update the review \n",
    "\n",
    "splitted_df['tokenized_review'] = data_lemmatized    \n",
    "splitted_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Positive and Negative Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "\n",
    "pos_lexicon = 'dict/positive-words.txt'\n",
    "neg_lexicon = 'dict/negative-words.txt'\n",
    "inc_words = 'dict/increment-words.txt'\n",
    "dec_words = 'dict/decrement-words.txt'\n",
    "inv_words = 'dict/inverse-words.txt'\n",
    "\n",
    "\n",
    "# Read the positive sentiment lexicon.\n",
    "pos_dict = {}\n",
    "f = open(pos_lexicon, 'r', encoding = \"ISO-8859-1\")\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    pos_dict[line] = 1\n",
    "f.close()\n",
    "\n",
    "# Read the negative sentiment lexicon.\n",
    "neg_dict = {}\n",
    "f = open(neg_lexicon, 'r', encoding = \"ISO-8859-1\")\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    neg_dict[line] = 1\n",
    "f.close()\n",
    "\n",
    "\n",
    "# Read the increment words.\n",
    "inc_dict = {}\n",
    "f = open(inc_words, 'r', encoding = \"ISO-8859-1\")\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    inc_dict[line] = 1\n",
    "f.close()\n",
    "\n",
    "# Read the decrement words.\n",
    "dec_dict = {}\n",
    "f = open(dec_words, 'r', encoding = \"ISO-8859-1\")\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    dec_dict[line] = 1\n",
    "f.close()\n",
    "\n",
    "# Read the inverse words.\n",
    "inv_dict = {}\n",
    "f = open(inv_words, 'r', encoding = \"ISO-8859-1\")\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    inv_dict[line] = 1\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# review_comment_text = \"extremely poor service\"\n",
    "\n",
    "# review_1 = word_tokenize(review_comment_text) \n",
    "# # review = [nltk.word_tokenize(w)for w in review_comment_text]\n",
    "# #review = [pos_tag(w) for w in review]\n",
    "\n",
    "# # tagsText= nltk.pos_tag(review)\n",
    "# print(review_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review_clean = [review_1]\n",
    "\n",
    "# for sent in review_clean:\n",
    "#     total_score = 0\n",
    "#     for w in sent:\n",
    "#         print(w)\n",
    "#         score = 0\n",
    "#         # If the word w is inside the positive lexicon, then increase the score by 1.\n",
    "#         if w in pos_dict:\n",
    "#             score = 1\n",
    "#         elif w in neg_dict:\n",
    "#             score = -1\n",
    "#         position = sent.index(w)\n",
    "#         previous_word = sent[position-1]\n",
    "#         if previous_word is not None:\n",
    "#             if previous_word in inc_dict : \n",
    "#                 print(score)\n",
    "#                 score *= 2.0\n",
    "#             elif previous_word in dec_dict:\n",
    "#                 print(score)\n",
    "#                 score /=2\n",
    "#             elif previous_word in inv_dict:\n",
    "#                 score *= -1  \n",
    "#         total_score+=score\n",
    "#         print(score)\n",
    "#         print(total_score)\n",
    "#     print(total_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = []\n",
    "score_label=[]\n",
    "\n",
    "for sent in data_lemmatized:\n",
    "    total_score = 0\n",
    "    print(sent)\n",
    "    for w in sent:\n",
    "        score = 0\n",
    "        if w in pos_dict:\n",
    "            score = 1\n",
    "        elif w in neg_dict:\n",
    "            score = -1\n",
    "        #check previous word\n",
    "        position = sent.index(w)\n",
    "        previous_word = sent[position-1]\n",
    "        if previous_word is not None:\n",
    "            if previous_word in inc_dict : \n",
    "                score *= 2.0\n",
    "            elif previous_word in dec_dict:\n",
    "                score /=2\n",
    "            elif previous_word in inv_dict:\n",
    "                score *= -1  \n",
    "        total_score+=score\n",
    "    \n",
    "    if total_score >0:\n",
    "        predicted_labels.append('1')\n",
    "        \n",
    "    elif total_score <0:\n",
    "        predicted_labels.append('-1')\n",
    "    else:\n",
    "        predicted_labels.append('0')\n",
    "        \n",
    "    score_label.append(int(total_score))\n",
    "    print(total_score)\n",
    "\n",
    "splitted_df[\"Polarity\"] = predicted_labels\n",
    "splitted_df[\"Score\"] = score_label\n",
    "splitted_df.to_csv('Lazada_sentiment.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
