{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a CNN Model to test Polarity\n",
    "### Reference: https://realpython.com/python-keras-text-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import similarities\n",
    "from gensim import models\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fast deliveri bubbl wrap', 'yet tri', 'chooos colour dull dont like use anymor', 'get flash deal still worth price', 'ship took one week', 'pretti mix color wear lip heh thank', 'pack leak', 'handi travel', 'good deal consid brand', 'packag arriv sooner expect well packag well', 'wait test', 'overal good buy tri product', 'deliveri realli fast', 'pack small item fuss free neat', 'howev product realli sticki', 'tri colour particular one sticki even use makeup remov lip dri still sticki', 'prolli wont use', 'next day deliveri', 'fast ship excel product', 'worth buy', 'receiv good condit', 'arriv super day order', 'product cake well protect', 'fast deliveri got flash', 'deliv next say', 'fast delivedi item good condit', 'worth smell realli', 'smell good colour strong', 'receiv super quick nice', 'receiv fast', 'item authent', 'yet tri', 'bought sale good discount', 'pretti fast', 'receiv within work day bubbl wrap envelop', 'colour onlin imag true life', 'fast item receiv good condit', 'yet tri', 'pack bubbl wrap', 'fast deliveri', 'skin love', 'product pretti good howev accident bought lighter tone', 'would definit buy hope next right shade', 'fast order sunday night receiv tuesday even', 'worth buy', 'good price good product', 'deliveri happi deal', 'fast', 'repeat order go liquid foundat mani year alreadi', 'cheaper mall price flash deal', 'item nice seal', 'deliveri smooth quick', 'fast glass bottl packag realli well didnt defect', 'bad product', 'fast deliveri', 'expect free mascara say say flash sale', 'well pack receiv good hope deal', 'deliveri took quit order got', 'order flash deal receiv end month deliveri time took realli long', 'good deal purchas sale', 'fast', 'good deliveri took week cheap sale', 'got flash good price', 'order sale deliveri took awhil well wrap came free mascara', 'nice packag fast deliveri', 'order sale took day arriv', 'item arriv quickli great condit', 'receiv good', 'love came', 'fast deliveri', 'item good condit', 'deliv doorstep bubbl wrap packag', 'rli like foundat far', 'receiv fast', 'item describ', 'fast', 'love fit well bit liquidi', 'good coverag', 'bought bottl sale bottl almost half retail', 'got foundat discount rate', 'brand correct order shade', 'thank', 'item ship day order deliv day', 'fast well pack', 'got safe', 'good buy', 'happi buyer', 'receiv day order', 'seal bubbl wrap packag', 'price cheaper retail bought plu', 'quick confirm deliveri', 'product mfg date', 'item also cheaper compar one sold drugstor', 'receiv item good condit', 'rcved parcel next day via door step deliveri', 'star caus pore still visibl applic', 'price cheaper compar giant drug store', 'well', 'color realli suit skin tone', 'far best afford foundat ever tri', 'deliv foundat', 'per advertis', 'mascara even includ', 'well receiv', 'happi free mascara along', 'item came pack bubbl wrap', 'use', 'came nice packag protect bottl', 'made whole lot easier use', 'definit bargain', 'product came day', 'gotten free mascara', 'great valu money one fave foundat ever', 'arriv well pack', 'deliveri super quick', 'item arriv perfect', 'super fast deliveri', 'order yesterday receiv today', 'even tri pump', 'fast deliveri', 'receiv time', 'tri new shade', 'bought flash deal', 'good buy plu come small gift', 'heard good review product', 'thank freebi', 'good deliveri door step', 'bought never tri', 'hope', 'good use', 'fast', 'product good', 'came free mascara great deal', 'great price flash', 'buy next time', 'receiv good condit', 'fast deliveri', 'deliveri pretti fast', 'receiv within week', 'well pack', 'come free mascara', 'receiv good condit', 'fast seller', 'receiv hope work', 'bought wrong', 'tri refund', 'fast deliveri', 'foundat came seal', 'good pump', 'deliveri took way longer expect domest deliveri week', 'receiv good condit', 'bad ship', 'order receiv', 'bottl new seal', 'one quickest deliv item order shopp', 'got less day upon place order', 'delight fast turnaround', 'fast deliveri', 'item came describ', 'fast deliveri', 'packag', 'product well pack bubbl bag', 'good coverag nice colour tone', 'skin feel refresh', 'good buy', 'wait week receiv item', 'bought yesterday receiv today', '', 'receiv follow day', 'manufactur date month ago', 'great seller', 'realli fast deliveri', 'look yellow tone pictur real life', 'deliveri awhil', 'get free gift though', 'worth', '', 'tri pack properli', 'order', 'order came good condit bubbl packag', 'thank free bottl makeup', 'look like one seen guardian watson', 'glad buy offer', 'super fast within day', 'quit matt smooth', 'love', 'good foundat transfer', 'bought wrong shade dark', 'fast deliveri', 'come well packag', 'foundat pump', 'super fast deliveri', 'good deal', 'love new pump well', 'good', 'fast deliveri item goooooood', 'favourit liquid', 'glad amaz foundat final come pump', 'highli', 'omg love especi pump', 'item receiv good condit', 'super fast deliveri', 'receiv good condit bottl wrap bubbl wrap', 'return custom', 'yay love pump', 'item well receiv well wrap', 'definit purchas', 'item well packag fast deliveri within day', 'fast ship', 'cheaper retail buy', 'item receiv good', 'fast deliveri', 'exactli describ', 'use time', 'fast deliveri good condit', 'get makeup remov', 'fast deliveri', 'exact shade', 'fast deliveri', 'product well item receiv good condit', 'bought two promot', 'love durabl', 'repeat purchas seller', 'came time fast', 'item well fast', 'buy', 'nice packag well protect', 'time deliveri', 'well receiv product', 'nice colour stay', 'disappoint', 'deliveri took awhil', 'tri hand', 'colour stay well despit wash', 'got order came free make remov', 'pretti disappoint took day item deliv', 'love', 'thicker expect advertis transfer', 'love colour', 'bright vibrant', 'alway buy sale good price happi', 'fast order receiv', 'deliveri fast', 'competit price freebi came promot day', 'worth purchas', 'product good know', 'item took', 'fast deliveri well pack', 'receiv well wife', 'receiv oct good condit', 'colour true photo', 'bought discount', 'mayb put much next time', 'item receiv good condit', 'havent tri item cant comment qualiti', 'alreadi own', 'last realli longer relationship', 'speed lick lip', 'orangi red realli stay use waterproof make remov get', 'like', 'realli superstay', 'deliveri quiet fast', 'definit buy colour', 'also love key ring', 'fast order arriv next', 'super worth price', '', 'orangey shade', 'repeat order', 'deliveri fast usual', 'like crayon matt superstay matt', 'tri yet receiv product seal good', 'feel dri lip colour realli long last', 'authent', 'manufactur date earli', 'fast', 'happi purchas', 'would', 'fast', 'love matt new lippi', 'colour realli vibrant best realli long last', 'happi purchas also deliveri yet tri foundat', 'love lipstick', 'proven superstay', 'product look', 'love', 'item receiv bubbl wrap', 'fast deliveri', 'product came good condit proper packag', 'fast deliveri', 'love colour n much cheaper retail', 'colour realli stay whole day even food', 'fast prompt', 'fast deliveri came nice', 'fav lipstick time', 'last', 'im look buy', 'bought gift', 'receiv well time', 'opinion look yet', 'hope work fine', 'lipstick mfg date except free gift shade protector', 'love lipstick bought second pc', 'order came super fast', 'item came quickli', 'realli wait use', 'realli love rang lipstick', 'receiv day', 'good', 'good', 'got lipstick almost half', 'nice reddish brown shade', 'pigment', 'receiv product good condit fast deliveri', 'bought lot good review', 'major love superstay lipstick rang', 'legit last long time', 'excit tri new', 'nice colour shea butter scent', 'comfort feel', 'love color', 'applic abit hard', 'receiv earlier expect', 'valu money', 'nice', 'nice', 'stick cup', 'transfer well', 'purchas usag', 'love colour', 'fast deliveri', 'good product', 'order ship next day', 'highli recommend sale', 'serious last forev', 'fast deliveri', 'good product', 'purchas item sale', 'took like almost week arriv', 'love', 'item receiev good condit', 'nice', 'receiv good condit', 'last', 'fast deliveri', 'would give zero star', 'love colour', 'super long last', 'deliveri cane bubbl wrap', 'fast deliveri', 'item receiv earlier expect', 'love', 'expect', 'amaz stay power', 'repeat custom conceal work well', 'travel size product', 'bought bhalf friend', 'cant rate', 'never tri pdt', 'bad', 'fast deliveri', 'seller sent free gift', 'receiv order describ', 'good buy', 'fast deliveri', 'buy', 'brand', 'good deal', 'like spong applic', 'half retail size purchas n', 'love size make remov', 'favourit conceal', 'worth price paid plu', 'excel buy worth', 'receiv within week', 'buy an cheaper retail shop', 'mini makeup remov good travel', 'conceal first time tri', 'fast deliveri', 'purchas cheaper buy outlet', 'item came bubbl wrap', 'look authent', 'item receiv good condit', 'purchas', 'fast deliveri', 'buy', 'came w free mini size make remov good travel', 'fav conceal', 'poor level servic', 'hope good', 'fast deliveri', 'good valu deal', 'full coverag', 'deliv day', 'bought money highli', 'fast deliveri', 'wonder chang packag', 'hope bought fake', 'realli fast ship order oct receiv oct', 'fast deliveri', 'receiv', 'hope colour suit', 'fast deliveri', 'authent item', 'thank', 'fast deliveri', 'well pack', 'thank order', 'fast deliveri', 'swear maybellin age rewind conceal best conceal ever', 'deliveri fast', 'much cheaper', 'love definit repurchas', 'order receiv good condit got right shade', 'deliveri time also', 'receiv free gift', 'alway like maybellin', 'receiv good condit', 'receiv good condit', 'fast deliveri', 'fast thank', 'purchas', 'come makeup remov promis', 'deliveri also took long', 'product okay', 'fast deliveri', 'good product', 'best drugstor conceal', 'purchas', 'love', 'properli wrap', 'effici deliveri', 'exactli describ', 'use time', '', 'order yesterday receiv today fast deliveri', 'great product', 'love conceal', '', 'item well receiv', 'deliveri took day', 'bought flash sale', 'worth', 'manufactur date quit old compar one bought store month', 'fast deliveri got notif driver packag', 'tri', 'hope dark', 'bought promo period', 'good', 'expect item small', 'happi purchas', 'item receiv good condit', 'receiv free lipstick suppos come togeth conceal', 'disappoint', 'fast deliveri', 'item came week order', 'great conceal', 'deliveri took one week arriv', 'custom servic need time get back enquiri', 'love love love product', 'exact one store', 'packag came undamag', 'multipl buy differ occas', 'alway satisfi', 'packag upgrad', 'second time make purchas']\n"
     ]
    }
   ],
   "source": [
    "data_file = \"SHOPEE_MAYBELLINE_CLEAN_V2.csv\"\n",
    "data = pd.read_csv(data_file)\n",
    "data.columns = data.columns.str.strip().str.replace(\" \",\"_\")\n",
    "# data.info()\n",
    "# data.head()\n",
    "\n",
    "# data.drop(columns=['Brand','Category','Product_Name','Price','Reviewer','Product_Purchase','Ratings','Date_Of_Review','Response', 'Topic'])\n",
    "# review_list = data['Review'].tolist()\n",
    "# polarity_list = data['Polarity'].tolist()\n",
    "\n",
    "reviews = data['Review']\n",
    "# polarity = data['Polarity']\n",
    "# print (reviews)\n",
    "\n",
    "review_docs = []\n",
    "for each_reviews in reviews:\n",
    "    temp = each_reviews.split(\" \")\n",
    "    review_docs.append(temp)\n",
    "# print (review_docs)\n",
    "\n",
    "# Make sure all words are in lowercase\n",
    "reviews_lower = [[each_word.lower() for each_word in each_review] for each_review in review_docs]\n",
    "# print (reviews_lower)\n",
    "\n",
    "# Use regular expressions to keep only allphabetical words\n",
    "reviews_alpha = [[each_word for each_word in each_review if re.search('^[a-z]+$', each_word)] for each_review in reviews_lower]\n",
    "# print (reviews_alpha)\n",
    "\n",
    "# Remove stop words\n",
    "stop_list = stopwords.words('english')\n",
    "reviews_stop = [[each_word for each_word in each_review if each_word not in stop_list] for each_review in reviews_alpha]\n",
    "# print (reviews_stop)\n",
    "\n",
    "# Porter Stemming\n",
    "stemmer = PorterStemmer()\n",
    "reviews_stem = [[stemmer.stem(each_word) for each_word in each_review] for each_review in reviews_stop]\n",
    "# print (reviews_stem)\n",
    "\n",
    "all_data_cleaned = []\n",
    "for each_sentence in reviews_stem:\n",
    "    sentence = \"\"\n",
    "    for each_word in each_sentence:\n",
    "        sentence += each_word + \" \"\n",
    "    sentence = sentence[0:-1]\n",
    "    all_data_cleaned.append(sentence)\n",
    "print (all_data_cleaned)\n",
    "\n",
    "polarity_raw = data['Polarity']\n",
    "polarity_0_and_1 = []\n",
    "for each_polarity in polarity_raw:\n",
    "    if int(each_polarity) == int(\"0\"):\n",
    "        polarity_0_and_1.append(0.5)\n",
    "    if int(each_polarity) == int(\"-1\"):\n",
    "        polarity_0_and_1.append(int(0))\n",
    "    if int(each_polarity) == int(\"1\"):\n",
    "        polarity_0_and_1.append(int(1))\n",
    "# print (polarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Model - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  81.19658119658119\n"
     ]
    }
   ],
   "source": [
    "reviews = all_data_cleaned\n",
    "# polarity_negative1_and_1 = data['Polarity']\n",
    "polarity = data['Polarity']\n",
    "# print (polarity)\n",
    "# print (reviews)\n",
    "vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
    "vectorizer.fit(reviews)\n",
    "vectorizer.vocabulary_\n",
    "vectorizer.transform(reviews).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, polarity, test_size=0.25, random_state=42)\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train)\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "logRegClf = LogisticRegression()\n",
    "logRegClf.fit(X_train, y_train)\n",
    "logRegClfscore = logRegClf.score(X_test, y_test)\n",
    "\n",
    "print (\"Accuracy of Logistic Regression: \", logRegClfscore*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Model - Logistic Regression with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Multinomial Naive Bayes with TFIDF:  77.77777777777779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = all_data_cleaned\n",
    "# polarity_negative1_and_1 = data['Polarity']\n",
    "polarity = data['Polarity']\n",
    "# print (polarity)\n",
    "# print (reviews)\n",
    "\n",
    "logRegTFIDFclf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', LogisticRegression())])\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'tfidf__norm': ('l1', 'l2')}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, polarity, test_size=0.25, random_state=42)\n",
    "\n",
    "logRegTFIDFclf = GridSearchCV(logRegTFIDFclf, parameters, cv=10)\n",
    "logRegTFIDFclf.fit(X_train, y_train)\n",
    "logRegTFIDFscore = logRegTFIDFclf.score (X_test, y_test)\n",
    "\n",
    "print (\"Accuracy of Logistic Regression with TFIDF: \", logRegTFIDFscore*100)\n",
    "logRegTFIDFclf.best_params_\n",
    "# print(classification_report(y_test, mnbTFIDFclf.predict(X_test), digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Model - Naive Bayes (Multinomial/Gaussian)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Multinomial Naive Bayes:  81.19658119658119\n",
      "Accuracy of Bernoulli Naive Bayes:  74.35897435897436\n"
     ]
    }
   ],
   "source": [
    "reviews = all_data_cleaned\n",
    "# polarity_negative1_and_1 = data['Polarity']\n",
    "polarity = data['Polarity']\n",
    "# print (polarity)\n",
    "# print (reviews)\n",
    "vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
    "vectorizer.fit(reviews)\n",
    "vectorizer.vocabulary_\n",
    "vectorizer.transform(reviews).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, polarity, test_size=0.25, random_state=42)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train)\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "mnbClf = MultinomialNB()\n",
    "mnbClf.fit(X_train, y_train)\n",
    "mnbClfscore = mnbClf.score (X_test, y_test)\n",
    "\n",
    "print (\"Accuracy of Multinomial Naive Bayes: \", mnbClfscore*100)\n",
    "\n",
    "bnbClf = BernoulliNB()\n",
    "bnbClf.fit(X_train, y_train)\n",
    "bnbClfscore = bnbClf.score (X_test, y_test)\n",
    "\n",
    "print (\"Accuracy of Bernoulli Naive Bayes: \", bnbClfscore*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Model - Multinomial NB with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Multinomial Naive Bayes with TFIDF:  82.90598290598291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.1,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__use_idf': True,\n",
       " 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = all_data_cleaned\n",
    "# polarity_negative1_and_1 = data['Polarity']\n",
    "polarity = data['Polarity']\n",
    "# print (polarity)\n",
    "# print (reviews)\n",
    "\n",
    "mnbTFIDFclf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())])\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'tfidf__norm': ('l1', 'l2'),\n",
    "              'clf__alpha': [1, 1e-1, 1e-2]}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, polarity, test_size=0.25, random_state=42)\n",
    "\n",
    "mnbTFIDFclf = GridSearchCV(mnbTFIDFclf, parameters, cv=10)\n",
    "mnbTFIDFclf.fit(X_train, y_train)\n",
    "mnbTFIDFscore = mnbTFIDFclf.score (X_test, y_test)\n",
    "\n",
    "print (\"Accuracy of Multinomial Naive Bayes with TFIDF: \", mnbTFIDFscore*100)\n",
    "mnbTFIDFclf.best_params_\n",
    "# print(classification_report(y_test, mnbTFIDFclf.predict(X_test), digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Model - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 540 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM:  82.05128205128204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1620 out of 1620 | elapsed:    7.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 3, 'degree': 1, 'gamma': 0.1, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = all_data_cleaned\n",
    "# polarity_negative1_and_1 = data['Polarity']\n",
    "polarity = data['Polarity']\n",
    "# print (polarity)\n",
    "# print (reviews)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
    "vectorizer.fit(reviews)\n",
    "vectorizer.vocabulary_\n",
    "vectorizer.transform(reviews).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, polarity, test_size=0.25, random_state=42)\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train)\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "parameters = {'C':[1,2,3,4,5,6,7,8,14], \n",
    "              'gamma':[0.1, 0.01, 0.001, 0.0001], \n",
    "              'kernel':['linear', 'poly', 'rbf'], \n",
    "              'degree': [1,2,3,4,5]}\n",
    "\n",
    "svmClf = GridSearchCV(param_grid = parameters, \n",
    "                      estimator= SVC(), \n",
    "                      scoring='accuracy', \n",
    "                      refit= True, \n",
    "                      verbose=1)\n",
    "\n",
    "svmClf.fit(X_train, y_train)\n",
    "svmClfscore = svmClf.score(X_test, y_test)\n",
    "\n",
    "print ('Accuracy of SVM: ', svmClfscore*100)\n",
    "svmClf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Model - SVM with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM with TFIDF:  77.77777777777779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = all_data_cleaned\n",
    "# polarity_negative1_and_1 = data['Polarity']\n",
    "polarity = data['Polarity']\n",
    "# print (polarity)\n",
    "# print (reviews)\n",
    "\n",
    "svmTFIDFclf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SVC(C=3, degree=1, gamma=0.1, kernel='rbf'))])\n",
    "\n",
    "# parameters = {'C':[1,2,3,4,5,6,7,8,14], \n",
    "#               'gamma':[0.1, 0.01, 0.001, 0.0001], \n",
    "#               'kernel':['linear', 'poly', 'rbf'], \n",
    "#               'degree': [1,2,3,4,5]}\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'tfidf__norm': ('l1', 'l2')}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, polarity, test_size=0.25, random_state=42)\n",
    "\n",
    "svmTFIDFclf = GridSearchCV(svmTFIDFclf, parameters, cv=10)\n",
    "svmTFIDFclf.fit(X_train, y_train)\n",
    "svmTFIDFscore = svmTFIDFclf.score(X_test, y_test)\n",
    "\n",
    "print (\"Accuracy of SVM with TFIDF: \", svmTFIDFscore*100)\n",
    "svmTFIDFclf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Model - SVM with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'fast deliveri bubbl wrap'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-5645c6da2c48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprincipalComponents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         \"\"\"\n\u001b[1;32m--> 360\u001b[1;33m         \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m         \u001b[0mU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mU\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\pca.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         X = check_array(X, dtype=[np.float64, np.float32], ensure_2d=True,\n\u001b[1;32m--> 382\u001b[1;33m                         copy=self.copy)\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[1;31m# Handle n_components==None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    494\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m                 \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    497\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'fast deliveri bubbl wrap'"
     ]
    }
   ],
   "source": [
    "reviews = all_data_cleaned\n",
    "# polarity_negative1_and_1 = data['Polarity']\n",
    "polarity = data['Polarity']\n",
    "# print (polarity)\n",
    "# print (reviews)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
    "vectorizer.fit(reviews)\n",
    "vectorizer.vocabulary_\n",
    "vectorizer.transform(reviews).toarray()\n",
    "\n",
    "pca = PCA(n_components = 10)\n",
    "principalComponents = pca.fit_transform(reviews)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, polarity, test_size=0.25, random_state=42)\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train)\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "parameters = {'C':[1,2,3,4,5,6,7,8,14], \n",
    "              'gamma':[0.1, 0.01, 0.001, 0.0001], \n",
    "              'kernel':['linear', 'poly', 'rbf'], \n",
    "              'degree': [1,2,3,4,5]}\n",
    "\n",
    "svmClf = GridSearchCV(param_grid = parameters, \n",
    "                      estimator= SVC(), \n",
    "                      scoring='accuracy', \n",
    "                      refit= True, \n",
    "                      verbose=1)\n",
    "\n",
    "svmClf.fit(X_train, y_train)\n",
    "svmClfscore = svmClf.score(X_test, y_test)\n",
    "\n",
    "print ('Accuracy of SVM: ', svmClfscore*100)\n",
    "svmClf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Model - ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = all_data_cleaned\n",
    "polarity = polarity_0_and_1\n",
    "# polarity = data['Polarity']\n",
    "vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
    "vectorizer.fit(reviews)\n",
    "vectorizer.vocabulary_\n",
    "vectorizer.transform(reviews).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, polarity, test_size=0.25, random_state=1000)\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train)\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "input_dim = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n",
      "Train on 349 samples, validate on 117 samples\n",
      "Epoch 1/50\n",
      "349/349 [==============================] - 0s 263us/sample - loss: 0.1347 - accuracy: 0.8281 - val_loss: 0.4128 - val_accuracy: 0.7692\n",
      "Epoch 2/50\n",
      "349/349 [==============================] - 0s 270us/sample - loss: 0.1163 - accuracy: 0.8281 - val_loss: 0.4277 - val_accuracy: 0.7607\n",
      "Epoch 3/50\n",
      "349/349 [==============================] - 0s 333us/sample - loss: 0.1022 - accuracy: 0.8309 - val_loss: 0.4526 - val_accuracy: 0.7778\n",
      "Epoch 4/50\n",
      "349/349 [==============================] - 0s 418us/sample - loss: 0.0919 - accuracy: 0.8367 - val_loss: 0.4667 - val_accuracy: 0.7692\n",
      "Epoch 5/50\n",
      "349/349 [==============================] - 0s 306us/sample - loss: 0.0847 - accuracy: 0.8395 - val_loss: 0.4756 - val_accuracy: 0.7778\n",
      "Epoch 6/50\n",
      "349/349 [==============================] - 0s 347us/sample - loss: 0.0773 - accuracy: 0.8424 - val_loss: 0.4969 - val_accuracy: 0.7778\n",
      "Epoch 7/50\n",
      "349/349 [==============================] - 0s 350us/sample - loss: 0.0746 - accuracy: 0.8395 - val_loss: 0.5102 - val_accuracy: 0.7778\n",
      "Epoch 8/50\n",
      "349/349 [==============================] - 0s 332us/sample - loss: 0.0680 - accuracy: 0.8424 - val_loss: 0.5361 - val_accuracy: 0.7863\n",
      "Epoch 9/50\n",
      "349/349 [==============================] - 0s 349us/sample - loss: 0.0639 - accuracy: 0.8395 - val_loss: 0.5549 - val_accuracy: 0.7863\n",
      "Epoch 10/50\n",
      "349/349 [==============================] - 0s 358us/sample - loss: 0.0610 - accuracy: 0.8424 - val_loss: 0.5683 - val_accuracy: 0.7863\n",
      "Epoch 11/50\n",
      "349/349 [==============================] - 0s 327us/sample - loss: 0.0576 - accuracy: 0.8424 - val_loss: 0.5830 - val_accuracy: 0.7778\n",
      "Epoch 12/50\n",
      "349/349 [==============================] - 0s 306us/sample - loss: 0.0552 - accuracy: 0.8424 - val_loss: 0.5962 - val_accuracy: 0.7692\n",
      "Epoch 13/50\n",
      "349/349 [==============================] - 0s 324us/sample - loss: 0.0532 - accuracy: 0.8424 - val_loss: 0.6237 - val_accuracy: 0.7778\n",
      "Epoch 14/50\n",
      "349/349 [==============================] - 0s 295us/sample - loss: 0.0509 - accuracy: 0.8424 - val_loss: 0.6238 - val_accuracy: 0.7778\n",
      "Epoch 15/50\n",
      "349/349 [==============================] - 0s 289us/sample - loss: 0.0497 - accuracy: 0.8424 - val_loss: 0.6371 - val_accuracy: 0.7778\n",
      "Epoch 16/50\n",
      "349/349 [==============================] - 0s 327us/sample - loss: 0.0486 - accuracy: 0.8453 - val_loss: 0.6553 - val_accuracy: 0.7778\n",
      "Epoch 17/50\n",
      "349/349 [==============================] - 0s 346us/sample - loss: 0.0482 - accuracy: 0.8424 - val_loss: 0.6681 - val_accuracy: 0.7778\n",
      "Epoch 18/50\n",
      "349/349 [==============================] - 0s 332us/sample - loss: 0.0467 - accuracy: 0.8453 - val_loss: 0.6834 - val_accuracy: 0.7778\n",
      "Epoch 19/50\n",
      "349/349 [==============================] - 0s 316us/sample - loss: 0.0456 - accuracy: 0.8424 - val_loss: 0.6840 - val_accuracy: 0.7778\n",
      "Epoch 20/50\n",
      "349/349 [==============================] - 0s 325us/sample - loss: 0.0445 - accuracy: 0.8453 - val_loss: 0.6995 - val_accuracy: 0.7778\n",
      "Epoch 21/50\n",
      "349/349 [==============================] - 0s 318us/sample - loss: 0.0432 - accuracy: 0.8453 - val_loss: 0.7138 - val_accuracy: 0.7778\n",
      "Epoch 22/50\n",
      "349/349 [==============================] - 0s 381us/sample - loss: 0.0420 - accuracy: 0.8453 - val_loss: 0.7162 - val_accuracy: 0.7778\n",
      "Epoch 23/50\n",
      "349/349 [==============================] - 0s 295us/sample - loss: 0.0410 - accuracy: 0.8453 - val_loss: 0.7260 - val_accuracy: 0.7778\n",
      "Epoch 24/50\n",
      "349/349 [==============================] - 0s 318us/sample - loss: 0.0421 - accuracy: 0.8453 - val_loss: 0.7378 - val_accuracy: 0.7778\n",
      "Epoch 25/50\n",
      "349/349 [==============================] - 0s 352us/sample - loss: 0.0411 - accuracy: 0.8424 - val_loss: 0.7480 - val_accuracy: 0.7778\n",
      "Epoch 26/50\n",
      "349/349 [==============================] - 0s 364us/sample - loss: 0.0387 - accuracy: 0.8453 - val_loss: 0.7584 - val_accuracy: 0.7778\n",
      "Epoch 27/50\n",
      "349/349 [==============================] - 0s 338us/sample - loss: 0.0390 - accuracy: 0.8453 - val_loss: 0.7710 - val_accuracy: 0.7778\n",
      "Epoch 28/50\n",
      "349/349 [==============================] - 0s 307us/sample - loss: 0.0377 - accuracy: 0.8453 - val_loss: 0.7749 - val_accuracy: 0.7778\n",
      "Epoch 29/50\n",
      "349/349 [==============================] - 0s 281us/sample - loss: 0.0373 - accuracy: 0.8424 - val_loss: 0.7885 - val_accuracy: 0.7778\n",
      "Epoch 30/50\n",
      "349/349 [==============================] - 0s 315us/sample - loss: 0.0370 - accuracy: 0.8424 - val_loss: 0.7941 - val_accuracy: 0.7778\n",
      "Epoch 31/50\n",
      "349/349 [==============================] - 0s 310us/sample - loss: 0.0366 - accuracy: 0.8424 - val_loss: 0.7987 - val_accuracy: 0.7778\n",
      "Epoch 32/50\n",
      "349/349 [==============================] - 0s 350us/sample - loss: 0.0360 - accuracy: 0.8453 - val_loss: 0.8165 - val_accuracy: 0.7778\n",
      "Epoch 33/50\n",
      "349/349 [==============================] - 0s 344us/sample - loss: 0.0358 - accuracy: 0.8424 - val_loss: 0.8187 - val_accuracy: 0.7778\n",
      "Epoch 34/50\n",
      "349/349 [==============================] - 0s 355us/sample - loss: 0.0351 - accuracy: 0.8453 - val_loss: 0.8242 - val_accuracy: 0.7778\n",
      "Epoch 35/50\n",
      "349/349 [==============================] - 0s 344us/sample - loss: 0.0349 - accuracy: 0.8453 - val_loss: 0.8370 - val_accuracy: 0.7778\n",
      "Epoch 36/50\n",
      "349/349 [==============================] - 0s 309us/sample - loss: 0.0344 - accuracy: 0.8453 - val_loss: 0.8394 - val_accuracy: 0.7778\n",
      "Epoch 37/50\n",
      "349/349 [==============================] - 0s 298us/sample - loss: 0.0347 - accuracy: 0.8424 - val_loss: 0.8455 - val_accuracy: 0.7778\n",
      "Epoch 38/50\n",
      "349/349 [==============================] - 0s 309us/sample - loss: 0.0340 - accuracy: 0.8453 - val_loss: 0.8605 - val_accuracy: 0.7778\n",
      "Epoch 39/50\n",
      "349/349 [==============================] - 0s 312us/sample - loss: 0.0343 - accuracy: 0.8424 - val_loss: 0.8666 - val_accuracy: 0.7778\n",
      "Epoch 40/50\n",
      "349/349 [==============================] - 0s 335us/sample - loss: 0.0332 - accuracy: 0.8453 - val_loss: 0.8685 - val_accuracy: 0.7778\n",
      "Epoch 41/50\n",
      "349/349 [==============================] - 0s 313us/sample - loss: 0.0330 - accuracy: 0.8424 - val_loss: 0.8784 - val_accuracy: 0.7778\n",
      "Epoch 42/50\n",
      "349/349 [==============================] - 0s 310us/sample - loss: 0.0333 - accuracy: 0.8453 - val_loss: 0.8885 - val_accuracy: 0.7778\n",
      "Epoch 43/50\n",
      "349/349 [==============================] - 0s 309us/sample - loss: 0.0334 - accuracy: 0.8453 - val_loss: 0.8984 - val_accuracy: 0.7778\n",
      "Epoch 44/50\n",
      "349/349 [==============================] - 0s 301us/sample - loss: 0.0326 - accuracy: 0.8424 - val_loss: 0.9005 - val_accuracy: 0.7778\n",
      "Epoch 45/50\n",
      "349/349 [==============================] - 0s 307us/sample - loss: 0.0329 - accuracy: 0.8453 - val_loss: 0.9287 - val_accuracy: 0.7778\n",
      "Epoch 46/50\n",
      "349/349 [==============================] - 0s 315us/sample - loss: 0.0310 - accuracy: 0.8453 - val_loss: 0.9128 - val_accuracy: 0.7778\n",
      "Epoch 47/50\n",
      "349/349 [==============================] - 0s 314us/sample - loss: 0.0319 - accuracy: 0.8424 - val_loss: 0.9272 - val_accuracy: 0.7778\n",
      "Epoch 48/50\n",
      "349/349 [==============================] - 0s 308us/sample - loss: 0.0332 - accuracy: 0.8453 - val_loss: 0.9437 - val_accuracy: 0.7778\n",
      "Epoch 49/50\n",
      "349/349 [==============================] - 0s 310us/sample - loss: 0.0311 - accuracy: 0.8424 - val_loss: 0.9343 - val_accuracy: 0.7778\n",
      "Epoch 50/50\n",
      "349/349 [==============================] - 0s 292us/sample - loss: 0.0312 - accuracy: 0.8424 - val_loss: 0.9518 - val_accuracy: 0.7778\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    # create model\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(16, input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    model.add(layers.Dense(8, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(layers.Dense(2, activation='softmax'))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.optimizers.Adam(), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "num_epochs = 50\n",
    "batch_size = 10\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs = num_epochs, validation_data = (X_test, y_test), batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n",
      "Accuracy of ANN: 77.77777910232544\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "ANNscore = accuracy\n",
    "print (\"Accuracy of ANN:\", ANNscore*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of All Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression:  81.19658119658119\n",
      "Accuracy of Logistic Regression with TFIDF:  77.77777777777779\n",
      "Accuracy of Multinomial Naive Bayes:  81.19658119658119\n",
      "Accuracy of Multinomial Naive Bayes with TFIDF:  82.90598290598291\n",
      "Accuracy of Bernoulli Naive Bayes:  74.35897435897436\n",
      "Accuracy of SVM:  82.05128205128204\n",
      "Accuracy of SVM with TFIDF:  77.77777777777779\n",
      "Accuracy of ANN: 77.77777910232544\n"
     ]
    }
   ],
   "source": [
    "print (\"Accuracy of Logistic Regression: \", logRegClfscore*100)\n",
    "print (\"Accuracy of Logistic Regression with TFIDF: \", logRegTFIDFscore*100)\n",
    "print (\"Accuracy of Multinomial Naive Bayes: \", mnbClfscore*100)\n",
    "print (\"Accuracy of Multinomial Naive Bayes with TFIDF: \", mnbTFIDFscore*100)\n",
    "print (\"Accuracy of Bernoulli Naive Bayes: \", bnbClfscore*100)\n",
    "print ('Accuracy of SVM: ', svmClfscore*100)\n",
    "print (\"Accuracy of SVM with TFIDF: \", svmTFIDFscore*100)\n",
    "print (\"Accuracy of ANN:\", ANNscore*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline Model: Naive Bayes - so we tried both Multinomial and Bernoulli\n",
    "Then tried a few more models like logistic regression, SVM, ..., ...\n",
    "Then tried MNB with TFIDF\n",
    "\n",
    "can try PCA word2vec doc2vec\n",
    "\n",
    "Why TFIDF reduce accuracy? https://datascience.stackexchange.com/questions/13660/in-general-when-does-tf-idf-reduce-accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Model - First Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                3110      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 55        \n",
      "=================================================================\n",
      "Total params: 3,165\n",
      "Trainable params: 3,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# print (type(reviews))\n",
    "\n",
    "reviews = all_data_cleaned\n",
    "polarity = polarity_0_and_1\n",
    "# polarity = data['Polarity']\n",
    "vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
    "vectorizer.fit(reviews)\n",
    "vectorizer.vocabulary_\n",
    "vectorizer.transform(reviews).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, polarity, test_size=0.25, random_state=1000)\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train)\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "# print (input_dim)\n",
    "# input_shape = (len(X_train, ))\n",
    "# print (input_shape)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(10, input_dim = input_dim , activation='tanh'))\n",
    "# model.add(layers.Dense(10, input_dim = input_dim , activation='sigmoid'))\n",
    "# model.add(layers.Dense(1, input_dim = input_shape , activation='relu'))\n",
    "model.add(layers.Dense(5, activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.optimizers.Adam(), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n",
      "Train on 349 samples, validate on 117 samples\n",
      "Epoch 1/50\n",
      "349/349 [==============================] - 0s 892us/sample - loss: 1.5021 - accuracy: 0.4556 - val_loss: 1.4241 - val_accuracy: 0.6068\n",
      "Epoch 2/50\n",
      "349/349 [==============================] - 0s 278us/sample - loss: 1.2797 - accuracy: 0.7278 - val_loss: 1.2141 - val_accuracy: 0.7265\n",
      "Epoch 3/50\n",
      "349/349 [==============================] - 0s 310us/sample - loss: 1.0527 - accuracy: 0.7880 - val_loss: 1.0115 - val_accuracy: 0.7350\n",
      "Epoch 4/50\n",
      "349/349 [==============================] - 0s 304us/sample - loss: 0.8517 - accuracy: 0.7822 - val_loss: 0.8563 - val_accuracy: 0.7350\n",
      "Epoch 5/50\n",
      "349/349 [==============================] - 0s 338us/sample - loss: 0.7062 - accuracy: 0.7794 - val_loss: 0.7516 - val_accuracy: 0.7350\n",
      "Epoch 6/50\n",
      "349/349 [==============================] - 0s 321us/sample - loss: 0.6070 - accuracy: 0.7851 - val_loss: 0.6806 - val_accuracy: 0.7350\n",
      "Epoch 7/50\n",
      "349/349 [==============================] - 0s 304us/sample - loss: 0.5383 - accuracy: 0.7851 - val_loss: 0.6300 - val_accuracy: 0.7350\n",
      "Epoch 8/50\n",
      "349/349 [==============================] - 0s 296us/sample - loss: 0.4861 - accuracy: 0.7937 - val_loss: 0.5920 - val_accuracy: 0.7521\n",
      "Epoch 9/50\n",
      "349/349 [==============================] - 0s 292us/sample - loss: 0.4440 - accuracy: 0.8023 - val_loss: 0.5613 - val_accuracy: 0.7521\n",
      "Epoch 10/50\n",
      "349/349 [==============================] - 0s 269us/sample - loss: 0.4067 - accuracy: 0.8052 - val_loss: 0.5351 - val_accuracy: 0.7607\n",
      "Epoch 11/50\n",
      "349/349 [==============================] - 0s 315us/sample - loss: 0.3754 - accuracy: 0.8109 - val_loss: 0.5098 - val_accuracy: 0.7521\n",
      "Epoch 12/50\n",
      "349/349 [==============================] - 0s 346us/sample - loss: 0.3463 - accuracy: 0.8166 - val_loss: 0.4902 - val_accuracy: 0.7521\n",
      "Epoch 13/50\n",
      "349/349 [==============================] - 0s 323us/sample - loss: 0.3208 - accuracy: 0.8252 - val_loss: 0.4746 - val_accuracy: 0.7521\n",
      "Epoch 14/50\n",
      "349/349 [==============================] - 0s 242us/sample - loss: 0.2977 - accuracy: 0.8281 - val_loss: 0.4602 - val_accuracy: 0.7692\n",
      "Epoch 15/50\n",
      "349/349 [==============================] - 0s 284us/sample - loss: 0.2771 - accuracy: 0.8281 - val_loss: 0.4478 - val_accuracy: 0.7778\n",
      "Epoch 16/50\n",
      "349/349 [==============================] - 0s 341us/sample - loss: 0.2584 - accuracy: 0.8252 - val_loss: 0.4389 - val_accuracy: 0.7778\n",
      "Epoch 17/50\n",
      "349/349 [==============================] - 0s 651us/sample - loss: 0.2421 - accuracy: 0.8309 - val_loss: 0.4300 - val_accuracy: 0.7863\n",
      "Epoch 18/50\n",
      "349/349 [==============================] - 0s 607us/sample - loss: 0.2269 - accuracy: 0.8309 - val_loss: 0.4257 - val_accuracy: 0.7863\n",
      "Epoch 19/50\n",
      "349/349 [==============================] - 0s 223us/sample - loss: 0.2138 - accuracy: 0.8309 - val_loss: 0.4212 - val_accuracy: 0.7949\n",
      "Epoch 20/50\n",
      "349/349 [==============================] - 0s 186us/sample - loss: 0.2020 - accuracy: 0.8309 - val_loss: 0.4202 - val_accuracy: 0.7949\n",
      "Epoch 21/50\n",
      "349/349 [==============================] - 0s 295us/sample - loss: 0.1913 - accuracy: 0.8338 - val_loss: 0.4165 - val_accuracy: 0.7863\n",
      "Epoch 22/50\n",
      "349/349 [==============================] - 0s 361us/sample - loss: 0.1813 - accuracy: 0.8338 - val_loss: 0.4168 - val_accuracy: 0.7863\n",
      "Epoch 23/50\n",
      "349/349 [==============================] - 0s 356us/sample - loss: 0.1726 - accuracy: 0.8338 - val_loss: 0.4163 - val_accuracy: 0.7863\n",
      "Epoch 24/50\n",
      "349/349 [==============================] - 0s 356us/sample - loss: 0.1647 - accuracy: 0.8338 - val_loss: 0.4178 - val_accuracy: 0.7863\n",
      "Epoch 25/50\n",
      "349/349 [==============================] - 0s 338us/sample - loss: 0.1574 - accuracy: 0.8338 - val_loss: 0.4183 - val_accuracy: 0.7778\n",
      "Epoch 26/50\n",
      "349/349 [==============================] - 0s 341us/sample - loss: 0.1508 - accuracy: 0.8338 - val_loss: 0.4216 - val_accuracy: 0.7778\n",
      "Epoch 27/50\n",
      "349/349 [==============================] - 0s 338us/sample - loss: 0.1443 - accuracy: 0.8338 - val_loss: 0.4246 - val_accuracy: 0.7778\n",
      "Epoch 28/50\n",
      "349/349 [==============================] - 0s 343us/sample - loss: 0.1390 - accuracy: 0.8367 - val_loss: 0.4268 - val_accuracy: 0.7778\n",
      "Epoch 29/50\n",
      "349/349 [==============================] - 0s 352us/sample - loss: 0.1340 - accuracy: 0.8367 - val_loss: 0.4304 - val_accuracy: 0.7778\n",
      "Epoch 30/50\n",
      "349/349 [==============================] - 0s 292us/sample - loss: 0.1293 - accuracy: 0.8367 - val_loss: 0.4341 - val_accuracy: 0.7778\n",
      "Epoch 31/50\n",
      "349/349 [==============================] - 0s 347us/sample - loss: 0.1248 - accuracy: 0.8338 - val_loss: 0.4372 - val_accuracy: 0.7692\n",
      "Epoch 32/50\n",
      "349/349 [==============================] - 0s 301us/sample - loss: 0.1205 - accuracy: 0.8338 - val_loss: 0.4429 - val_accuracy: 0.7778\n",
      "Epoch 33/50\n",
      "349/349 [==============================] - 0s 280us/sample - loss: 0.1169 - accuracy: 0.8338 - val_loss: 0.4460 - val_accuracy: 0.7778\n",
      "Epoch 34/50\n",
      "349/349 [==============================] - 0s 347us/sample - loss: 0.1138 - accuracy: 0.8367 - val_loss: 0.4526 - val_accuracy: 0.7778\n",
      "Epoch 35/50\n",
      "349/349 [==============================] - 0s 319us/sample - loss: 0.1099 - accuracy: 0.8338 - val_loss: 0.4548 - val_accuracy: 0.7778\n",
      "Epoch 36/50\n",
      "349/349 [==============================] - 0s 356us/sample - loss: 0.1068 - accuracy: 0.8367 - val_loss: 0.4586 - val_accuracy: 0.7778\n",
      "Epoch 37/50\n",
      "349/349 [==============================] - 0s 278us/sample - loss: 0.1038 - accuracy: 0.8395 - val_loss: 0.4645 - val_accuracy: 0.7778\n",
      "Epoch 38/50\n",
      "349/349 [==============================] - 0s 495us/sample - loss: 0.1010 - accuracy: 0.8395 - val_loss: 0.4703 - val_accuracy: 0.7778\n",
      "Epoch 39/50\n",
      "349/349 [==============================] - 0s 491us/sample - loss: 0.0988 - accuracy: 0.8424 - val_loss: 0.4773 - val_accuracy: 0.7778\n",
      "Epoch 40/50\n",
      "349/349 [==============================] - 0s 347us/sample - loss: 0.0959 - accuracy: 0.8424 - val_loss: 0.4806 - val_accuracy: 0.7778\n",
      "Epoch 41/50\n",
      "349/349 [==============================] - 0s 598us/sample - loss: 0.0936 - accuracy: 0.8424 - val_loss: 0.4848 - val_accuracy: 0.7778\n",
      "Epoch 42/50\n",
      "349/349 [==============================] - 0s 369us/sample - loss: 0.0918 - accuracy: 0.8424 - val_loss: 0.4924 - val_accuracy: 0.7778\n",
      "Epoch 43/50\n",
      "349/349 [==============================] - 0s 315us/sample - loss: 0.0898 - accuracy: 0.8424 - val_loss: 0.4954 - val_accuracy: 0.7778\n",
      "Epoch 44/50\n",
      "349/349 [==============================] - 0s 533us/sample - loss: 0.0878 - accuracy: 0.8424 - val_loss: 0.4999 - val_accuracy: 0.7778\n",
      "Epoch 45/50\n",
      "349/349 [==============================] - 0s 639us/sample - loss: 0.0858 - accuracy: 0.8424 - val_loss: 0.5045 - val_accuracy: 0.7778\n",
      "Epoch 46/50\n",
      "349/349 [==============================] - 0s 364us/sample - loss: 0.0841 - accuracy: 0.8424 - val_loss: 0.5129 - val_accuracy: 0.7778\n",
      "Epoch 47/50\n",
      "349/349 [==============================] - 0s 301us/sample - loss: 0.0825 - accuracy: 0.8424 - val_loss: 0.5168 - val_accuracy: 0.7778\n",
      "Epoch 48/50\n",
      "349/349 [==============================] - 0s 390us/sample - loss: 0.0812 - accuracy: 0.8424 - val_loss: 0.5231 - val_accuracy: 0.7778\n",
      "Epoch 49/50\n",
      "349/349 [==============================] - 0s 450us/sample - loss: 0.0792 - accuracy: 0.8424 - val_loss: 0.5285 - val_accuracy: 0.7778\n",
      "Epoch 50/50\n",
      "349/349 [==============================] - 0s 195us/sample - loss: 0.0780 - accuracy: 0.8424 - val_loss: 0.5334 - val_accuracy: 0.7778\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "batch_size = 10\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs = num_epochs, validation_data = (X_test, y_test), batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n",
      "Training Accuracy: 0.8424\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n",
      "Testing Accuracy: 0.7778\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy: {:.4f}\".format(accuracy))\n",
    "\n",
    "### num_epochs = 100\n",
    "# tanh > softmax == 0.7863\n",
    "# sigmoid > softmax == 0.7778\n",
    "# tanh > sigmoid > softmax == 0.7778\n",
    "# tanh > relu > softmax == 0.7607\n",
    "# tanh > tanh > softmax == 0.7607\n",
    "# relu > tanh > softmax == 0.7521\n",
    "\n",
    "### num_epochs = 50\n",
    "# tanh > softmax == 0.7863\n",
    "# sigmoid > softmax == 0.7778\n",
    "# tanh > sigmoid > softmax == 0.7949\n",
    "# tanh > relu > softmax == 0.7863\n",
    "# tanh > tanh > softmax == 0.7436\n",
    "# relu > tanh > softmax == 0.7778"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Model - with Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = all_data_cleaned\n",
    "# print (reviews)\n",
    "polarity = polarity_0_and_1\n",
    "# print (polarity)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, polarity, test_size=0.25, random_state=1000)\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "# y_train= tokenizer.texts_to_sequences(y_train)\n",
    "# y_test = tokenizer.texts_to_sequences(y_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# print (X_train[0:5])\n",
    "# print (X_train_num[0:5])\n",
    "\n",
    "maxlen = 100\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 50)           15600     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                50010     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 65,621\n",
      "Trainable params: 65,621\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "model2 = tf.keras.Sequential()\n",
    "model2.add(layers.Embedding(input_dim = vocab_size,\n",
    "                            output_dim = embedding_dim,\n",
    "                            input_length = maxlen))\n",
    "model2.add(layers.Flatten())\n",
    "model2.add(layers.Dense(10, activation='relu'))\n",
    "model2.add(layers.Dense(1, activation='sigmoid'))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer=tf.optimizers.Adam(), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'float'>\", \"<class 'int'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-7e811baccd7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mhistory2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m           distribution_strategy=strategy)\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    495\u001b[0m                      'at same time.')\n\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 497\u001b[1;33m   \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m   \u001b[1;31m# Handle validation_split, we want to split the data and get the training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    651\u001b[0m         \u001b[1;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m         \"input: {}, {}\".format(\n\u001b[1;32m--> 653\u001b[1;33m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[0;32m    654\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'float'>\", \"<class 'int'>\"})"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "batch_size = 10\n",
    "\n",
    "history2 = model2.fit(X_train, y_train, epochs = num_epochs, validation_data = (X_test, y_test), batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Model - Using Pretrained Word Embeddings\n",
    "#### Download the GloVe\n",
    "#### Another alternative is to train your own word embeddings with the gemsim python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 2273: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-28eb3769897b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0membedding_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0membedding_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_embedding_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'glove.6B/glove.6B.50d.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-d40ca2a1b491>\u001b[0m in \u001b[0;36mcreate_embedding_matrix\u001b[1;34m(filepath, word_index, embedding_dim)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mvector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 2273: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "embedding_matrix = create_embedding_matrix('glove.6B/glove.6B.50d.txt', tokenizer.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-9d7838dc7247>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnonzero_elements\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcovered_vocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnonzero_elements\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcovered_vocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embedding_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "covered_vocabulary = nonzero_elements / vocab_size\n",
    "print (covered_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Model - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 310, 50)           15600     \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 15500)             0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10)                155010    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 11        \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 5)                 10        \n",
      "=================================================================\n",
      "Total params: 170,631\n",
      "Trainable params: 170,631\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# print (type(reviews))\n",
    "\n",
    "reviews = all_data_cleaned\n",
    "polarity = polarity_0_and_1\n",
    "# polarity = data['Polarity']\n",
    "vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
    "vectorizer.fit(reviews)\n",
    "vectorizer.vocabulary_\n",
    "vectorizer.transform(reviews).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, polarity, test_size=0.25, random_state=1000)\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train)\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print (vocab_size)\n",
    "maxlen = 310\n",
    "# X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "# X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "embedding_dim = 50\n",
    "# print (input_dim)\n",
    "# input_shape = (len(X_train, ))\n",
    "# print (input_shape)\n",
    "modelCNN = tf.keras.Sequential()\n",
    "modelCNN.add(layers.Embedding(input_dim = vocab_size,\n",
    "                           output_dim = embedding_dim,\n",
    "                           input_length = maxlen))\n",
    "modelCNN.add(layers.Flatten())\n",
    "# modelCNN.add(layers.GlobalMaxPool1D())\n",
    "modelCNN.add(layers.Dense(10, activation='relu'))\n",
    "modelCNN.add(layers.Dense(1, activation='sigmoid'))\n",
    "# model.add(layers.Dense(10, input_dim = input_dim , activation='sigmoid'))\n",
    "# model.add(layers.Dense(1, input_dim = input_shape , activation='relu'))\n",
    "modelCNN.add(layers.Dense(5, activation = 'softmax'))\n",
    "modelCNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCNN.compile(optimizer=tf.optimizers.Adam(), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "history = modelCNN.fit(X_train, y_train,\n",
    "                    epochs=50,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
